{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df8aeb80-e0bc-446a-af96-7c5c591da418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Shape: (1276, 100305)\n",
      "Converted to NumPy. Shape: (1276, 100304)\n",
      "Adjusted sequence length: 800\n",
      "Data reshaped to: (1276, 800, 4)\n",
      "Source and target created. src shape: (1276, 799, 4) , tgt shape: (1276, 799, 4)\n",
      "Data prepared and moved to device.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --------------------------\n",
    "# Load and preprocess data\n",
    "# --------------------------\n",
    "\n",
    "# Load transformed CSV\n",
    "df = pd.read_csv(\"output/maestro_transformed.csv\")\n",
    "print(\"Data loaded. Shape:\", df.shape)\n",
    "\n",
    "# Drop 'song_id' column (not needed for training)\n",
    "df = df.drop(columns=[\"song_id\"])\n",
    "\n",
    "# Convert DataFrame to NumPy array\n",
    "data = df.to_numpy()\n",
    "print(f\"Converted to NumPy. Shape: {data.shape}\")\n",
    "\n",
    "# Ensure correct sequence length (each song has timestamps * 4 features)\n",
    "input_dim = 4  # Features per timestamp\n",
    "sequence_length = data.shape[1] // input_dim  # Dynamically calculate timestamps per sample\n",
    "\n",
    "# You can set a fixed number of timestamps to reduce memory usage.\n",
    "# For example, limit the sequence length to 300 timestamps.\n",
    "max_sequence_length = 500  # Adjust this based on your system's capability\n",
    "sequence_length = min(sequence_length, max_sequence_length)  # Ensure it doesn't exceed the max length\n",
    "\n",
    "print(f\"Adjusted sequence length: {sequence_length}\")\n",
    "\n",
    "# Reshape data to (num_samples, sequence_length, input_dim)\n",
    "data = data[:, :sequence_length * input_dim]  # Trim or pad data to match the new sequence length\n",
    "data = data.reshape(data.shape[0], sequence_length, input_dim)\n",
    "print(\"Data reshaped to:\", data.shape)\n",
    "\n",
    "# ------------------------------\n",
    "# Prepare for Auto-Regressive Training\n",
    "# ------------------------------\n",
    "\n",
    "# Use first T-1 tokens as source and the last T-1 as target.\n",
    "src = data[:, :-1, :]  # (num_samples, sequence_length-1, 4)\n",
    "tgt = data[:, 1:, :]   # (num_samples, sequence_length-1, 4)\n",
    "print(\"Source and target created. src shape:\", src.shape, \", tgt shape:\", tgt.shape)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(src, tgt, test_size=0.2, random_state=12)\n",
    "\n",
    "# Convert to PyTorch tensors and move to device\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "print(\"Data prepared and moved to device.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e802e6e9-2cff-4db8-b57f-c9e78fc039fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample src: [[0.52873563 0.00139645 0.00257095 0.40944882]\n",
      " [0.5862069  0.00253395 0.00270782 0.52755906]\n",
      " [0.52873563 0.00309808 0.00448898 0.30708661]\n",
      " [0.45977011 0.00309438 0.00460736 0.30708661]\n",
      " [0.65517241 0.00304999 0.00511415 0.51181102]]\n",
      "Sample tgt: [[0.5862069  0.00253395 0.00270782 0.52755906]\n",
      " [0.52873563 0.00309808 0.00448898 0.30708661]\n",
      " [0.45977011 0.00309438 0.00460736 0.30708661]\n",
      " [0.65517241 0.00304999 0.00511415 0.51181102]\n",
      " [0.57471264 0.00305924 0.00614253 0.35433071]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample src:\", src[0, :5, :])  # First 5 timestamps\n",
    "print(\"Sample tgt:\", tgt[0, :5, :])  # Next 5 timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3406d8b-4c6e-45d2-bfa2-44b29cfdf670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------\n",
    "# Positional Encoding Class\n",
    "# --------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# --------------------------\n",
    "# Transformer Model\n",
    "# --------------------------\n",
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, max_seq_len, output_dim, dropout=0.2):\n",
    "        super(MusicTransformer, self).__init__()\n",
    "        \n",
    "        # Embedding layer to project input features to model dimension\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.pos_encoder = PositionalEncoding(model_dim, max_seq_len, dropout)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim, nhead=num_heads, dropout=dropout, batch_first=True,activation=nn.LeakyReLU(negative_slope=0.01))\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Transformer Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=model_dim, nhead=num_heads, dropout=dropout, batch_first=True, activation=nn.LeakyReLU(negative_slope=0.01))\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Final linear layer\n",
    "        self.fc_out = nn.Linear(model_dim, output_dim)\n",
    "        \n",
    "        # Layer normalization for stability\n",
    "        self.norm = nn.LayerNorm(model_dim)\n",
    "\n",
    "    def forward(self, src, tgt, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        src: (batch_size, seq_len, input_dim)\n",
    "        tgt: (batch_size, seq_len, input_dim)\n",
    "        tgt_mask: (seq_len, seq_len) optional, causal mask for decoder\n",
    "        \"\"\"\n",
    "        # Embed input sequences\n",
    "        src = self.pos_encoder(self.embedding(src))  # (batch_size, seq_len, model_dim)\n",
    "        tgt = self.pos_encoder(self.embedding(tgt))\n",
    "\n",
    "        # Encode source sequence\n",
    "        memory = self.transformer_encoder(self.norm(src))\n",
    "\n",
    "        # If no tgt_mask is provided, generate one\n",
    "        if tgt_mask is None:\n",
    "            tgt_mask = self.generate_square_subsequent_mask(tgt.size(1), tgt.device)\n",
    "\n",
    "        # Decode the target sequence using the encoder memory\n",
    "        output = self.transformer_decoder(self.norm(tgt), memory, tgt_mask)\n",
    "        \n",
    "        # Project output back to the original feature dimension\n",
    "        return self.fc_out(output)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_square_subsequent_mask(size, device):\n",
    "        \"\"\"Generates a causal mask to prevent attending to future tokens.\"\"\"\n",
    "        return torch.triu(torch.full((size, size), float('-inf'), device=device), diagonal=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062df291-1844-4824-9461-ea6b21fd6614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96e659df-f3b1-4bd1-8869-6cb1011b6a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varadkulkarni/miniconda3/envs/projectR/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MusicTransformer(\n",
      "  (embedding): Linear(in_features=4, out_features=128, bias=True)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (activation): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer_decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(p=0.2, inplace=False)\n",
      "        (activation): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc_out): Linear(in_features=128, out_features=4, bias=True)\n",
      "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "No previous weights found. Starting training from scratch.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "# --------------------------\n",
    "# Model, Loss, and Optimizer\n",
    "# --------------------------\n",
    "# Model parameters\n",
    "model_dim = 128\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "output_dim = input_dim  # For auto-regression\n",
    "\n",
    "max_seq_len = sequence_length  # Ensure this matches the expected input length\n",
    "\n",
    "# Initialize the model with batch_first=True\n",
    "model = MusicTransformer(input_dim, model_dim, num_heads, num_layers, max_seq_len, output_dim).to(device)\n",
    "print(model)\n",
    "\n",
    "# Create DataLoaders (batch size of 32)\n",
    "BATCH_SIZE = 16\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()  \n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-5)  # Adjusted T_max\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.8)\n",
    "\n",
    "# --------------------------\n",
    "# Checkpoint Setup\n",
    "# --------------------------\n",
    "os.makedirs('output', exist_ok=True)\n",
    "model_name = f\"best_model_dim{model_dim}_heads{num_heads}_layers{num_layers}.pth\"\n",
    "checkpoint_path = f'output/{model_name}'\n",
    "\n",
    "# Initialize training state variables\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "early_stopping_patience = 100\n",
    "loss_improvement_threshold = 1e-4  \n",
    "\n",
    "# Load checkpoint if exists\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading model checkpoint from {checkpoint_path}...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)  # Ensures compatibility across devices\n",
    "    \n",
    "    saved_model_dim = checkpoint.get('model_dim')\n",
    "    saved_num_heads = checkpoint.get('num_heads')\n",
    "    saved_num_layers = checkpoint.get('num_layers')\n",
    "\n",
    "    if (saved_model_dim == model_dim and saved_num_heads == num_heads and saved_num_layers == num_layers):\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "        epochs_without_improvement = checkpoint.get('epochs_without_improvement', 0)\n",
    "        print(\"Checkpoint loaded successfully. Continuing training.\")\n",
    "    else:\n",
    "        print(\"Architecture mismatch detected. Initializing a new model with the new configuration.\")\n",
    "else:\n",
    "    print(\"No previous weights found. Starting training from scratch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d775f41-ff6f-45ab-88cf-5eaed448758f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9056861-878e-4c09-b90a-b11c122ce0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Initialize TensorBoard\n",
    "#writer = SummaryWriter(\"runs/music_transformer_experiment\")\n",
    "\n",
    "# --------------------------\n",
    "# Training Loop\n",
    "# --------------------------\n",
    "num_epochs = 500\n",
    "\n",
    "\n",
    "print(\"Training started\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    for src_batch, tgt_batch in train_loader:\n",
    "        src_batch, tgt_batch = src_batch.to(device), tgt_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        decoder_input = tgt_batch[:, :-1, :]\n",
    "        ground_truth = tgt_batch[:, 1:, :]\n",
    "\n",
    "        tgt_mask = model.generate_square_subsequent_mask(decoder_input.size(1), device)\n",
    "\n",
    "        output = model(src_batch, decoder_input, tgt_mask=tgt_mask)\n",
    "        loss = criterion(output, ground_truth)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Log Gradients & Weights to TensorBoard after computing gradients\n",
    "        \"\"\"for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                writer.add_histogram(f\"Gradients/{name}\", param.grad.cpu().detach(), epoch)\n",
    "            writer.add_histogram(f\"Weights/{name}\", param.cpu().detach().numpy(), epoch)\"\"\"\n",
    "        #for name, param in model.named_parameters():\n",
    "            #if param.requires_grad:\n",
    "                #print(f\"{name} - Mean: {param.data.mean()}, Std: {param.data.std()}\")\n",
    "        #for name, param in model.named_parameters():\n",
    "            #if param.grad is not None:\n",
    "                #print(f\"{name} - Gradient mean: {param.grad.mean()}, Gradient std: {param.grad.std()}\")\n",
    "\n",
    "\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # --------------------------\n",
    "    # Validation Phase\n",
    "    # --------------------------\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for src_batch, tgt_batch in val_loader:\n",
    "            src_batch, tgt_batch = src_batch.to(device), tgt_batch.to(device)\n",
    "\n",
    "            decoder_input = tgt_batch[:, :-1, :]\n",
    "            ground_truth = tgt_batch[:, 1:, :]\n",
    "            tgt_mask = model.generate_square_subsequent_mask(decoder_input.size(1), device)\n",
    "\n",
    "            output = model(src_batch, decoder_input, tgt_mask=tgt_mask)\n",
    "            loss = criterion(output, ground_truth)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    # Log metrics to TensorBoard\n",
    "    #writer.add_scalar(\"Loss/Train\", avg_train_loss, epoch)\n",
    "    #writer.add_scalar(\"Loss/Validation\", avg_val_loss, epoch)\n",
    "    #writer.add_scalar(\"Learning Rate\", optimizer.param_groups[0][\"lr\"], epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Model Checkpointing\n",
    "    # --------------------------\n",
    "    if avg_val_loss < best_val_loss - loss_improvement_threshold:\n",
    "        improvement = best_val_loss - avg_val_loss\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_without_improvement = 0  # Reset counter\n",
    "\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'epochs_without_improvement': epochs_without_improvement,\n",
    "            'model_dim': model_dim,\n",
    "            'num_heads': num_heads,\n",
    "            'num_layers': num_layers\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Saved improved model. Improvement: {improvement:.6f}\")\n",
    "    \n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"No improvement. Epochs without improvement: {epochs_without_improvement}\")\n",
    "\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "            break\n",
    "\n",
    "# Close TensorBoard writer\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c742b8-2797-420f-b1b1-1beae73a296c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8295848-e8e4-44d1-943a-d5d1f888280b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0015904a-f841-44b1-97fe-514d16fbf461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de6d51e-700a-46a1-8d57-b1eaff54935d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8666c8a8-9bdb-4a0f-8502-84d4fe923d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab994d3-dbdf-40ae-818c-2880aadf4818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a114a357-10af-489a-ae71-bd02c5db251c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last loss from checkpoint: 0.006994853261858225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cx/4yp83sjj55bbc_tjxb3tc9bc0000gn/T/ipykernel_73152/4197669831.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_checkpoint = torch.load('output/best_model_dim256_heads8_layers4.pth')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "model_checkpoint = torch.load('output/best_model_dim256_heads8_layers4.pth')\n",
    "\n",
    "# Check if 'loss' is stored in the checkpoint dictionary\n",
    "if 'best_val_loss' in model_checkpoint:\n",
    "    print(f\"Last loss from checkpoint: {model_checkpoint['best_val_loss']}\")\n",
    "else:\n",
    "    print(\"Loss is not stored in the checkpoint.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a35d5aad-21a3-4f40-b877-34859c837dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d831e1-0050-45de-9bb0-3a962ddf562d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22f87017-3617-4e45-85f2-1458564a63d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varadkulkarni/miniconda3/envs/projectR/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True\n",
      "  warnings.warn(\n",
      "/var/folders/cx/4yp83sjj55bbc_tjxb3tc9bc0000gn/T/ipykernel_70366/2267863081.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated_midi_end_time 430.8352864583333\n",
      "Seed Sequence: tensor([[[4.4828e-01, 4.6421e-04, 9.2843e-04, 5.0394e-01],\n",
      "         [4.9425e-01, 1.1605e-03, 1.6248e-03, 6.2992e-01],\n",
      "         [5.2874e-01, 1.8569e-03, 2.3211e-04, 7.8740e-01],\n",
      "         [5.8621e-01, 2.5532e-04, 3.0174e-04, 9.4488e-01]]])\n",
      "Generated music saved to 'output/generated_music_flat.csv'.\n",
      "Denormalization complete.\n",
      "Denormalized music saved to 'output/denormalized_music_flat.csv'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Define device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the trained model\n",
    "model_path = \"output/best_model_dim128_heads8_layers4.pth\"\n",
    "model = MusicTransformer(\n",
    "    input_dim=4,\n",
    "    model_dim=128,\n",
    "    num_heads=8,\n",
    "    num_layers=4,\n",
    "    max_seq_len=500,\n",
    "    output_dim=4\n",
    ").to(device)\n",
    "\n",
    "# Load model checkpoint\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Load MIDI normalization params\n",
    "with open(\"output/normalization_params.json\", \"r\") as file:\n",
    "    normalization_params = json.load(file)\n",
    "\n",
    "midi_end_times = list(normalization_params[\"midi_end_times\"].values())\n",
    "estimated_midi_end_time = np.median(midi_end_times) if midi_end_times else 60  # Default 60 sec\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Generate a Structured Seed Sequence\n",
    "# ==========================\n",
    "def generate_seed_sequence(estimated_midi_end_time):\n",
    "    \"\"\"Creates a simple, coherent starting melody pattern with discrete velocities.\n",
    "    \n",
    "    Args:\n",
    "        estimated_midi_end_time (float): The maximum end time used for normalization.\n",
    "                                         This should match the normalization applied to the training data.\n",
    "    \"\"\"\n",
    "    pitches = [60, 64, 67, 72]  # C4, E4, G4, C5\n",
    "    start_times = [0.2, 0.5, 0.8, 0.11]  # Start times in seconds\n",
    "    end_times = [0.4, 0.7, 0.10, 0.13]    # End times in seconds\n",
    "    velocities = [64, 80, 100, 120]     # Discrete MIDI velocities\n",
    "\n",
    "    # Normalize values\n",
    "    velocity_normalized = [v / 127 for v in velocities]  # Normalize velocity to [0, 1]\n",
    "    start_times_normalized = [s / estimated_midi_end_time for s in start_times]  # Normalize start times\n",
    "    end_times_normalized = [e / estimated_midi_end_time for e in end_times]      # Normalize end times\n",
    "\n",
    "    seed = np.array([\n",
    "        [(p - 21) / (108 - 21), s, e, v]  # Normalize pitch, start time, end time, velocity\n",
    "        for p, s, e, v in zip(pitches, start_times_normalized, end_times_normalized, velocity_normalized)\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    return torch.tensor(seed).unsqueeze(0).to(device)  # Shape: (1, seq_len, 4)\n",
    "\n",
    "# Initialize seed sequence\n",
    "seed_sequence = generate_seed_sequence(estimated_midi_end_time)\n",
    "print(\"estimated_midi_end_time\", estimated_midi_end_time)\n",
    "print(\"Seed Sequence:\", seed_sequence)\n",
    "\n",
    "# ==========================\n",
    "# Generate New Music with Temperature\n",
    "# ==========================\n",
    "generated_length = 493  # Total number of timesteps to generate (seed + new predictions)\n",
    "temperature = 0.8\n",
    "\n",
    "# Initialize generated_music with the seed sequence\n",
    "seed_np = seed_sequence.squeeze(0).cpu().numpy()  # Convert seed sequence to numpy array\n",
    "generated_music = [seed_np[i] for i in range(seed_np.shape[0])]  # Add seed sequence to generated_music\n",
    "\n",
    "# Start with the seed sequence\n",
    "current_sequence = seed_sequence\n",
    "\n",
    "# Generate new music\n",
    "with torch.no_grad():\n",
    "    for t in range(generated_length - seed_sequence.size(1)):  # Adjust loop length\n",
    "        src = current_sequence\n",
    "        \n",
    "        # Predict next timestep\n",
    "        output = model(src, src)[:, -1, :]  # Shape: (1, 4)\n",
    "        output = output.squeeze(0).cpu().numpy()\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        scaled_output = output / temperature\n",
    "\n",
    "        # Store generated step\n",
    "        predicted_timestep = np.array([\n",
    "            np.clip(scaled_output[0], 0, 1),  # Pitch (normalized)\n",
    "            np.clip(scaled_output[1], 0, 1),  # Start time\n",
    "            np.clip(scaled_output[2], 0, 1),  # End time\n",
    "            np.clip(scaled_output[3], 0, 1)   # Velocity (normalized)\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        generated_music.append(predicted_timestep)\n",
    "\n",
    "        # Append generated step to sequence\n",
    "        predicted_timestep_tensor = torch.tensor(predicted_timestep).unsqueeze(0).unsqueeze(0).to(device)\n",
    "        current_sequence = torch.cat((current_sequence, predicted_timestep_tensor), dim=1)\n",
    "\n",
    "# Convert generated music into a **single row** DataFrame\n",
    "flattened_values = np.array(generated_music).flatten()  # Convert to 1D array\n",
    "column_names = [f\"{feature}_{i+1}\" for i in range(generated_length) for feature in [\"pitch\", \"start\", \"end\", \"velocity\"]]\n",
    "\n",
    "df_flat = pd.DataFrame([flattened_values], columns=column_names)\n",
    "\n",
    "# Save normalized output\n",
    "normalized_csv = \"output/generated_music_flat.csv\"\n",
    "df_flat.to_csv(normalized_csv, index=False)\n",
    "print(f\"Generated music saved to '{normalized_csv}'.\")\n",
    "\n",
    "# =============================================================================\n",
    "# DENORMALIZATION\n",
    "# =============================================================================\n",
    "def denormalize_data(df, generated_length, estimated_midi_end_time):\n",
    "    \"\"\"Convert normalized values back to MIDI-compatible values.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    for i in range(generated_length):\n",
    "        df[f\"pitch_{i+1}\"] = (df[f\"pitch_{i+1}\"] * (108 - 21) + 21).clip(21, 108).round().astype(int)\n",
    "        df[f\"velocity_{i+1}\"] = (df[f\"velocity_{i+1}\"] * 127).clip(0, 127).round().astype(int)  # Make sure it's discrete\n",
    "        df[f\"start_{i+1}\"] *= estimated_midi_end_time\n",
    "        df[f\"end_{i+1}\"] *= estimated_midi_end_time\n",
    "\n",
    "    print(\"Denormalization complete.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load & denormalize generated music\n",
    "df_denormalized = denormalize_data(df_flat, generated_length, estimated_midi_end_time)\n",
    "\n",
    "# Save denormalized data\n",
    "denormalized_csv = \"output/denormalized_music_flat.csv\"\n",
    "df_denormalized.to_csv(denormalized_csv, index=False)\n",
    "print(f\"Denormalized music saved to '{denormalized_csv}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf67cc48-20f7-4e3c-86a8-ca9c4679c228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1870249c-5b81-427a-b138-ccbcfbe209af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pitch_1</th>\n",
       "      <th>start_1</th>\n",
       "      <th>end_1</th>\n",
       "      <th>velocity_1</th>\n",
       "      <th>pitch_2</th>\n",
       "      <th>start_2</th>\n",
       "      <th>end_2</th>\n",
       "      <th>velocity_2</th>\n",
       "      <th>pitch_3</th>\n",
       "      <th>start_3</th>\n",
       "      <th>...</th>\n",
       "      <th>end_491</th>\n",
       "      <th>velocity_491</th>\n",
       "      <th>pitch_492</th>\n",
       "      <th>start_492</th>\n",
       "      <th>end_492</th>\n",
       "      <th>velocity_492</th>\n",
       "      <th>pitch_493</th>\n",
       "      <th>start_493</th>\n",
       "      <th>end_493</th>\n",
       "      <th>velocity_493</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>80</td>\n",
       "      <td>67</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>43.168747</td>\n",
       "      <td>127</td>\n",
       "      <td>108</td>\n",
       "      <td>41.62538</td>\n",
       "      <td>41.97353</td>\n",
       "      <td>127</td>\n",
       "      <td>108</td>\n",
       "      <td>40.98625</td>\n",
       "      <td>41.1912</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 1972 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pitch_1  start_1  end_1  velocity_1  pitch_2  start_2  end_2  velocity_2  \\\n",
       "0       60      0.2    0.4          64       64      0.5    0.7          80   \n",
       "\n",
       "   pitch_3  start_3  ...    end_491  velocity_491  pitch_492  start_492  \\\n",
       "0       67      0.8  ...  43.168747           127        108   41.62538   \n",
       "\n",
       "    end_492  velocity_492  pitch_493  start_493  end_493  velocity_493  \n",
       "0  41.97353           127        108   40.98625  41.1912           127  \n",
       "\n",
       "[1 rows x 1972 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df=pd.read_csv(denormalized_csv)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c33fa22-0678-47fe-97cb-2c89e069d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def denormalized_to_midi(input_csv, output_dir, log_file=\"output/log.txt\", num_timestamps=493):\n",
    "    \"\"\"\n",
    "    Converts denormalized music data from a CSV file into MIDI files.\n",
    "    Each row represents a song, with per-note features: pitch, start, end, velocity.\n",
    "\n",
    "    Args:\n",
    "        input_csv (str): Path to the input CSV file with denormalized data.\n",
    "        output_dir (str): Directory to save output MIDI files.\n",
    "        log_file (str): Path to the log file for logs.\n",
    "        num_timestamps (int): Number of timestamps (notes) per song.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(input_csv)\n",
    "        os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists\n",
    "\n",
    "        # Prepare logging\n",
    "        log_messages = [f\"Processing started.\\nTotal Songs in CSV: {len(data)}\\n\"]\n",
    "\n",
    "        for idx, row in data.iterrows():\n",
    "            try:\n",
    "                midi = pretty_midi.PrettyMIDI()\n",
    "                instrument = pretty_midi.Instrument(program=0)  # Default: Acoustic Grand Piano\n",
    "                note_count = 0\n",
    "\n",
    "                for i in range(1, num_timestamps + 1):\n",
    "                    # Retrieve features\n",
    "                    pitch = row.get(f\"pitch_{i}\")\n",
    "                    start = row.get(f\"start_{i}\")\n",
    "                    end = row.get(f\"end_{i}\")\n",
    "                    velocity = row.get(f\"velocity_{i}\")\n",
    "\n",
    "                    # Ensure all values are valid numbers\n",
    "                    if pd.notna(pitch) and pd.notna(start) and pd.notna(end) and pd.notna(velocity):\n",
    "                        pitch = int(round(pitch))  # Ensure integer\n",
    "                        velocity = int(round(min(max(velocity, 0), 127)))  # Clip velocity\n",
    "                        start, end = float(start), float(end)  # Ensure float\n",
    "\n",
    "                        if 21 <= pitch <= 108 and start >= 0 and end > start:\n",
    "                            note = pretty_midi.Note(\n",
    "                                velocity=velocity,\n",
    "                                pitch=pitch,\n",
    "                                start=start,\n",
    "                                end=end\n",
    "                            )\n",
    "                            instrument.notes.append(note)\n",
    "                            note_count += 1\n",
    "\n",
    "                if note_count > 0:\n",
    "                    midi.instruments.append(instrument)\n",
    "                    output_midi_path = os.path.join(output_dir, f\"song_{idx + 1}.mid\")\n",
    "                    midi.write(output_midi_path)\n",
    "                    log_messages.append(f\"Song {idx + 1}: {note_count} notes added. MIDI saved at {output_midi_path}\")\n",
    "                else:\n",
    "                    log_messages.append(f\"Skipping song {idx + 1}: No valid notes found.\")\n",
    "\n",
    "            except Exception as row_error:\n",
    "                log_messages.append(f\"Error processing song {idx + 1}: {row_error}\")\n",
    "\n",
    "        # Save logs after processing all songs\n",
    "        with open(log_file, \"w\") as log:\n",
    "            log.write(\"\\n\".join(log_messages) + \"\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        with open(log_file, \"a\") as log:  # Append error logs instead of overwriting\n",
    "            log.write(f\"Error occurred: {e}\\n\")\n",
    "\n",
    "# Example Usage\n",
    "input_csv_path = \"output/denormalized_music_flat.csv\"\n",
    "output_midi_dir = \"output/midi_files\"\n",
    "denormalized_to_midi(input_csv_path, output_midi_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a093b839-f80b-4b34-aef6-f13d863c9699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.30.7, Python 3.12.0)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Playing output/midi_files/song_1.mid...\n",
      "Press Enter to stop playback.\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "\n",
    "def play_midi_with_stop(midi_file):\n",
    "    \"\"\"\n",
    "    Play a MIDI file and stop on user input.\n",
    "    \n",
    "    Args:\n",
    "        midi_file (str): Path to the MIDI file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pygame.mixer.init()\n",
    "        pygame.mixer.music.load(midi_file)\n",
    "        pygame.mixer.music.play()\n",
    "\n",
    "        print(f\"Playing {midi_file}...\")\n",
    "        print(\"Press Enter to stop playback.\")\n",
    "        input()  # Wait for user input to stop\n",
    "        pygame.mixer.music.stop()\n",
    "        print(\"Playback stopped.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        pygame.mixer.quit()\n",
    "\n",
    "# Example Usage\n",
    "midi_file_path = \"output/midi_files/song_1.mid\"\n",
    "play_midi_with_stop(midi_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
